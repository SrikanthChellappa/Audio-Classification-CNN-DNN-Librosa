{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a1b2817",
   "metadata": {},
   "source": [
    "# Audio Classification - Environmental Sounds - CNN-DNN-Librosa\n",
    "\n",
    "We are going to use a subset of the data from ESC-50 dataset from https://dagshub.com/kinkusuma/esc50-dataset. The ESC-50 dataset is a labeled collection of 2000 environmental audio recordings suitable for benchmarking methods of environmental sound classification.\n",
    "We will develop and train a model to classify 8 differnet environment sounds from the above dataset that has 50+ environment sound audio files for classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b016795d",
   "metadata": {},
   "source": [
    "# Common Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ff52b9eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential,load_model\n",
    "from tensorflow.keras.layers import Conv2D,MaxPooling2D,Flatten,Dense,Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.image import resize\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "import librosa\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a619fe",
   "metadata": {},
   "source": [
    "# Defining labels for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a8d0fa5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_data_path=r'D:\\Kaggle-Competitions\\Audio Classification\\Environmental-Sound-Classification\\data'\n",
    "inference_categories=os.listdir(audio_data_path)\n",
    "category_count=len(inference_categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c3ba53",
   "metadata": {},
   "source": [
    "# Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ced76f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess audio data\n",
    "def load_and_preprocess_data(data_dir, classes, target_shape=(200, 200)):\n",
    "    data = []\n",
    "    labels = []\n",
    "    \n",
    "    for i, class_name in enumerate(classes):\n",
    "        class_dir = os.path.join(data_dir, class_name)\n",
    "        for filename in os.listdir(class_dir):\n",
    "            if filename.endswith('.wav'):\n",
    "                file_path = os.path.join(class_dir, filename)\n",
    "                audio_data, sample_rate = librosa.load(file_path, sr=None)\n",
    "                # Perform preprocessing (e.g., convert to Mel spectrogram and resize)\n",
    "                mel_spectrogram = librosa.feature.melspectrogram(y=audio_data, sr=sample_rate)\n",
    "                mel_spectrogram = resize(np.expand_dims(mel_spectrogram, axis=-1), target_shape)\n",
    "                data.append(mel_spectrogram)\n",
    "                labels.append(i)\n",
    "    \n",
    "    return np.array(data), np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "94d11c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and testing sets\n",
    "data, labels = load_and_preprocess_data(audio_data_path, inference_categories)\n",
    "labels = to_categorical(labels, num_classes=len(inference_categories))  # Convert labels to one-hot encoding\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a6465f5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 200, 1)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "018ab785",
   "metadata": {},
   "source": [
    "# Defining callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7d628290",
   "metadata": {},
   "outputs": [],
   "source": [
    "class myCallback(Callback):\n",
    "  def on_epoch_end(self, epoch, logs={}):\n",
    "    '''\n",
    "    Halts the training when the loss falls below 0.1\n",
    "\n",
    "    Args:\n",
    "      epoch (integer) - index of epoch (required but unused in the function definition below)\n",
    "      logs (dict) - metric results from the training epoch\n",
    "    '''\n",
    "    # Check the loss\n",
    "    if(logs.get('loss') < 0.1):\n",
    "      # Stop if threshold is met\n",
    "      print(\"\\nLoss is lower than 0.1 so cancelling training!\")\n",
    "      self.model.stop_training = True\n",
    "                \n",
    "    \n",
    "# Instantiate class\n",
    "callbacks = myCallback()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f86d7a8",
   "metadata": {},
   "source": [
    "# Creating and compiling model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6ac75d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    model=Sequential([\n",
    "        Conv2D(64,(3,3),activation='relu',input_shape=X_train[0].shape),\n",
    "        MaxPooling2D(2,2),\n",
    "        Conv2D(128,(3,3),activation='relu'),\n",
    "        MaxPooling2D(2,2),\n",
    "        Conv2D(256,(3,3),activation='relu'),\n",
    "        MaxPooling2D(2,2),\n",
    "        Flatten(),\n",
    "        Dense(512,activation='relu'),\n",
    "        Dropout(0.3),\n",
    "        Dense(256,activation='relu'),\n",
    "        Dropout(0.3),\n",
    "        Dense(category_count,activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy',optimizer=Adam(),metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba3bb242",
   "metadata": {},
   "source": [
    "# Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "99727165",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "11/11 [==============================] - 22s 2s/step - loss: 20.4759 - accuracy: 0.3452 - val_loss: 1.7997 - val_accuracy: 0.4688\n",
      "Epoch 2/100\n",
      "11/11 [==============================] - 20s 2s/step - loss: 1.4997 - accuracy: 0.6071 - val_loss: 1.7927 - val_accuracy: 0.6250\n",
      "Epoch 3/100\n",
      "11/11 [==============================] - 20s 2s/step - loss: 1.1534 - accuracy: 0.7143 - val_loss: 1.5507 - val_accuracy: 0.6719\n",
      "Epoch 4/100\n",
      "11/11 [==============================] - 20s 2s/step - loss: 0.8736 - accuracy: 0.8214 - val_loss: 2.1977 - val_accuracy: 0.5469\n",
      "Epoch 5/100\n",
      "11/11 [==============================] - 20s 2s/step - loss: 0.6650 - accuracy: 0.8294 - val_loss: 1.8552 - val_accuracy: 0.5625\n",
      "Epoch 6/100\n",
      "11/11 [==============================] - 22s 2s/step - loss: 0.7305 - accuracy: 0.8730 - val_loss: 2.3905 - val_accuracy: 0.5938\n",
      "Epoch 7/100\n",
      "11/11 [==============================] - 22s 2s/step - loss: 0.5380 - accuracy: 0.8770 - val_loss: 2.0960 - val_accuracy: 0.5469\n",
      "Epoch 8/100\n",
      "11/11 [==============================] - 21s 2s/step - loss: 0.3582 - accuracy: 0.8929 - val_loss: 2.5023 - val_accuracy: 0.5938\n",
      "Epoch 9/100\n",
      "11/11 [==============================] - 20s 2s/step - loss: 0.3215 - accuracy: 0.9167 - val_loss: 2.7589 - val_accuracy: 0.6094\n",
      "Epoch 10/100\n",
      "11/11 [==============================] - 20s 2s/step - loss: 0.2303 - accuracy: 0.9484 - val_loss: 3.6921 - val_accuracy: 0.6250\n",
      "Epoch 11/100\n",
      "11/11 [==============================] - 20s 2s/step - loss: 0.5097 - accuracy: 0.9127 - val_loss: 3.0784 - val_accuracy: 0.5938\n",
      "Epoch 12/100\n",
      "11/11 [==============================] - 20s 2s/step - loss: 0.3609 - accuracy: 0.9365 - val_loss: 5.2458 - val_accuracy: 0.5625\n",
      "Epoch 13/100\n",
      "11/11 [==============================] - 21s 2s/step - loss: 0.4322 - accuracy: 0.9246 - val_loss: 3.7701 - val_accuracy: 0.5938\n",
      "Epoch 14/100\n",
      "11/11 [==============================] - 21s 2s/step - loss: 0.2292 - accuracy: 0.9405 - val_loss: 5.0200 - val_accuracy: 0.5625\n",
      "Epoch 15/100\n",
      "11/11 [==============================] - 20s 2s/step - loss: 0.1379 - accuracy: 0.9683 - val_loss: 5.3971 - val_accuracy: 0.6094\n",
      "Epoch 16/100\n",
      "11/11 [==============================] - 20s 2s/step - loss: 0.2174 - accuracy: 0.9484 - val_loss: 5.4898 - val_accuracy: 0.5938\n",
      "Epoch 17/100\n",
      "11/11 [==============================] - 21s 2s/step - loss: 0.4237 - accuracy: 0.9405 - val_loss: 4.6476 - val_accuracy: 0.5469\n",
      "Epoch 18/100\n",
      "11/11 [==============================] - 21s 2s/step - loss: 0.1659 - accuracy: 0.9524 - val_loss: 5.3218 - val_accuracy: 0.6562\n",
      "Epoch 19/100\n",
      "11/11 [==============================] - 20s 2s/step - loss: 0.1737 - accuracy: 0.9524 - val_loss: 6.5820 - val_accuracy: 0.6094\n",
      "Epoch 20/100\n",
      "11/11 [==============================] - 20s 2s/step - loss: 0.4329 - accuracy: 0.9325 - val_loss: 5.2814 - val_accuracy: 0.6094\n",
      "Epoch 21/100\n",
      "11/11 [==============================] - 20s 2s/step - loss: 0.5753 - accuracy: 0.9365 - val_loss: 3.5970 - val_accuracy: 0.6562\n",
      "Epoch 22/100\n",
      "11/11 [==============================] - 20s 2s/step - loss: 0.4774 - accuracy: 0.8929 - val_loss: 1.7496 - val_accuracy: 0.5469\n",
      "Epoch 23/100\n",
      "11/11 [==============================] - 20s 2s/step - loss: 0.3805 - accuracy: 0.9008 - val_loss: 2.1209 - val_accuracy: 0.6406\n",
      "Epoch 24/100\n",
      "11/11 [==============================] - 20s 2s/step - loss: 0.1973 - accuracy: 0.9484 - val_loss: 2.2955 - val_accuracy: 0.6250\n",
      "Epoch 25/100\n",
      "11/11 [==============================] - 20s 2s/step - loss: 0.2315 - accuracy: 0.9603 - val_loss: 4.0562 - val_accuracy: 0.6250\n",
      "Epoch 26/100\n",
      "11/11 [==============================] - 21s 2s/step - loss: 0.2162 - accuracy: 0.9524 - val_loss: 3.9112 - val_accuracy: 0.7031\n",
      "Epoch 27/100\n",
      "11/11 [==============================] - 21s 2s/step - loss: 0.1390 - accuracy: 0.9603 - val_loss: 4.3365 - val_accuracy: 0.6406\n",
      "Epoch 28/100\n",
      "11/11 [==============================] - 22s 2s/step - loss: 0.1010 - accuracy: 0.9722 - val_loss: 4.8241 - val_accuracy: 0.6406\n",
      "Epoch 29/100\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.0816 - accuracy: 0.9762\n",
      "Loss is lower than 0.1 so cancelling training!\n",
      "11/11 [==============================] - 23s 2s/step - loss: 0.0816 - accuracy: 0.9762 - val_loss: 4.8962 - val_accuracy: 0.6250\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x254d84b4810>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model=create_model()\n",
    "model.fit(X_train,y_train,epochs=100,verbose=1,batch_size=25,validation_data=(X_test,y_test),callbacks=[callbacks])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c17c5b6f",
   "metadata": {},
   "source": [
    "# Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5df0e10e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sudar\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\engine\\training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "#Validate the model on test dataset (X_test,y_test) \n",
    "test_accuracy=model.evaluate(X_test,y_test,verbose=0)\n",
    "print(test_accuracy[1])\n",
    "\n",
    "# Save the model\n",
    "model.save('audio_classification_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08218460",
   "metadata": {},
   "source": [
    "# Evaluate with a random .wav file data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "58ad22f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 156ms/step\n",
      "The audio is classified as: dog\n",
      "Accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# Load the saved model\n",
    "model = load_model('audio_classification_model.h5')\n",
    "\n",
    "# Define the target shape for input spectrograms\n",
    "target_shape = (200, 200)\n",
    "\n",
    "# Function to preprocess and classify an audio file\n",
    "def test_audio(file_path, model):\n",
    "    # Load and preprocess the audio file\n",
    "    audio_data, sample_rate = librosa.load(file_path, sr=None)\n",
    "    mel_spectrogram = librosa.feature.melspectrogram(y=audio_data, sr=sample_rate)\n",
    "    mel_spectrogram = resize(np.expand_dims(mel_spectrogram, axis=-1), target_shape)\n",
    "    mel_spectrogram = tf.reshape(mel_spectrogram, (1,) + target_shape + (1,))\n",
    "\n",
    "    # Make predictions\n",
    "    predictions = model.predict(mel_spectrogram)\n",
    "\n",
    "    # Get the class probabilities\n",
    "    class_probabilities = predictions[0]\n",
    "\n",
    "    # Get the predicted class index\n",
    "    predicted_class_index = np.argmax(class_probabilities)\n",
    "\n",
    "    return class_probabilities, predicted_class_index\n",
    "\n",
    "# Test an audio file\n",
    "test_audio_file = './tmp/audio-test1.wav'\n",
    "class_probabilities, predicted_class_index = test_audio(test_audio_file, model)\n",
    "\n",
    "# Display results for all classes\n",
    "for i, class_label in enumerate(inference_categories):\n",
    "    probability = class_probabilities[i]\n",
    "    #print(f'Class: {class_label}, Probability: {probability:.4f}')\n",
    "\n",
    "# Calculate and display the predicted class and accuracy\n",
    "predicted_class = inference_categories[predicted_class_index]\n",
    "accuracy = class_probabilities[predicted_class_index]\n",
    "print(f'The audio is classified as: {predicted_class}')\n",
    "print(f'Accuracy: {accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89eeca79",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfenv",
   "language": "python",
   "name": "tfenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
